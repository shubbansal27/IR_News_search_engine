{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e7fb94242fb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd C:\\Users\\FDUSER.M-1737.000\\dataset\\bbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mini_batch_generator(x, y, vocab, vocab_size, vocab_check, maxlen, batch_size=128):\n",
    "\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        x_sample = x[i:i + batch_size]\n",
    "        y_sample = y[i:i + batch_size]\n",
    "\n",
    "        input_data = encode_data(x_sample, maxlen, vocab, vocab_size,\n",
    "                                 vocab_check)\n",
    "\n",
    "        yield (input_data, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_data(x, maxlen, vocab, vocab_size, check):\n",
    "    #Iterate over the loaded data and create a matrix of size maxlen x vocabsize\n",
    "    #In this case that will be 1014x69. This is then placed in a 3D matrix of size\n",
    "    #data_samples x maxlen x vocab_size. Each character is encoded into a one-hot\n",
    "    #array. Chars not in the vocab are encoded into an all zero vector.\n",
    "\n",
    "    input_data = np.zeros((len(x), maxlen, vocab_size))\n",
    "    \n",
    "    for dix, sent in enumerate(x):\n",
    "        counter = 0\n",
    "        sent_array = np.zeros((maxlen, vocab_size))\n",
    "        chars = list(sent.lower().replace(' ', ''))\n",
    "        for c in chars:\n",
    "            if counter >= maxlen:\n",
    "                pass\n",
    "            else:\n",
    "                char_array = np.zeros(vocab_size, dtype=np.int)\n",
    "                if c in check:\n",
    "                    ix = vocab[c]\n",
    "                    char_array[ix] = 1\n",
    "                sent_array[counter, :] = char_array\n",
    "                counter += 1\n",
    "        input_data[dix, :, :] = sent_array\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function creates a vocab of characters.\n",
    "\n",
    "def create_vocab_set():\n",
    "    #This alphabet is 69 chars vs. 70 reported in the paper since they include two\n",
    "    # '-' characters. See https://github.com/zhangxiangxiao/Crepe#issues.\n",
    "\n",
    "    alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n",
    "                list(string.punctuation) + ['\\n'])\n",
    "    \n",
    "    vocab_size = len(alphabet)\n",
    "    \n",
    "    check = set(alphabet)\n",
    "\n",
    "    vocab = {}\n",
    "    reverse_vocab = {}\n",
    "    \n",
    "    for ix, t in enumerate(alphabet):\n",
    "        vocab[t] = ix\n",
    "        reverse_vocab[ix] = t\n",
    "\n",
    "    return vocab, reverse_vocab, vocab_size, check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_matrix2(x, y):\n",
    "    stacked = np.hstack((np.matrix(x).T, y))\n",
    "    xi = np.array(stacked[:, 0]).flatten()\n",
    "    yi = np.array(stacked[:, 1:])\n",
    "\n",
    "    return xi, yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model params\n",
    "\n",
    "#Filters for conv layers\n",
    "nb_filter = 256\n",
    "\n",
    "#Number of units in the dense layer\n",
    "dense_outputs = 1024\n",
    "\n",
    "#Conv layer kernel size\n",
    "filter_kernels = [7, 7, 3, 3, 3, 3]\n",
    "\n",
    "#Number of units in the final output layer. Number of classes.\n",
    "cat_output = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maindir = os.getcwd()\n",
    "tempDirList = [x[0] for x in os.walk(maindir)]\n",
    "dirList = [x[39:] for x in tempDirList if x[39:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "columns_list = ['Text', 'Class']\n",
    "\n",
    "data_df = pd.DataFrame(columns=columns_list, index = None)\n",
    "\n",
    "for dl in dirList:\n",
    "    path = maindir + '\\\\' + dl # use your path\n",
    "    allFiles = glob.glob(path + \"\\*.txt\")\n",
    "    for filename in allFiles:\n",
    "        with open(filename, 'r') as file_:\n",
    "            txt = file_.read()\n",
    "            data_df = data_df.append({'Text': txt, 'Class':dl}, ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df['News_Type'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = 0\n",
    "for cls in dirList:\n",
    "    data_df.loc[data_df['Class'] == cls, 'News_Type'] = val\n",
    "    val = val + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = data_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = data_df.drop('Class', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import re\n",
    "# Use regular expressions to do a find-and-replace\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_review) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 5. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_text = data_df[\"Text\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_text = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range(0,num_text):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_text.append(review_to_words(data_df[\"Text\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ag_data2(data_df):\n",
    "\n",
    "    # Get the number of reviews based on the dataframe column size\n",
    "    num_text = data_df[\"Text\"].size\n",
    "\n",
    "    # Initialize an empty list to hold the clean reviews\n",
    "    clean_train_text = []\n",
    "\n",
    "    # Loop over each review; create an index i that goes from 0 to the length\n",
    "    # of the movie review list \n",
    "    for i in range(0,num_text):\n",
    "        # Call our function for each one, and add the result to the list of\n",
    "        # clean reviews\n",
    "        clean_train_text.append(review_to_words(data_df[\"Text\"][i]))\n",
    "        \n",
    "    x_train = clean_train_text[:2000]\n",
    "    x_train = np.array(x_train)\n",
    "    \n",
    "    y_train = data_df['News_Type']\n",
    "    y_train = y_train[:2000]\n",
    "    y_train = to_categorical(y_train)\n",
    "    \n",
    "    x_test = clean_train_text[2000:]\n",
    "    x_test = np.array(x_test)\n",
    "    \n",
    "    y_test = data_df['News_Type']\n",
    "    y_test = y_test[2000:]\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(xt, yt), (x_test, y_test) = load_ag_data2(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab, reverse_vocab, vocab_size, check = create_vocab_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = encode_data(x_test, maxlen, vocab, vocab_size, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd C:\\\\Users\\\\FDUSER.M-1737.000\\\\dataset\\\\AGNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name_path = 'params\\\\AG_model3.json'\n",
    "model_weights_path = 'params\\\\AG_model_weights3.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(model_name_path, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "for layer in model.layers[:5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "dropout_2 = model.layers[-1].output\n",
    "#Output dense layer with softmax activation\n",
    "pred = Dense(cat_output, activation='softmax', name='output')(dropout_2)\n",
    "\n",
    "model = Model(input = model.input, output = pred)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'Adadelta' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile/fit params\n",
    "batch_size = 80\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Fit model...')\n",
    "\n",
    "for e in range(nb_epoch):\n",
    "    \n",
    "    xi, yi = shuffle_matrix(xt, yt)\n",
    "    \n",
    "    batches = mini_batch_generator(xi, yi, vocab, vocab_size, check, maxlen, batch_size=batch_size)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    loss = 0.0\n",
    "    step = 1\n",
    "    print('Epoch: {}'.format(e))\n",
    "    \n",
    "    for x_train, y_train in batches:\n",
    "        f = model.train_on_batch(x_train, y_train)\n",
    "        loss += f[0]\n",
    "        loss_avg = loss / step\n",
    "        accuracy += f[1]\n",
    "        accuracy_avg = accuracy / step\n",
    "        if step % 10 == 0:\n",
    "            print('  Step: {}'.format(step))\n",
    "            print('\\tLoss: {}. Accuracy: {}'.format(loss_avg, accuracy_avg))\n",
    "        step += 1\n",
    "\n",
    "print('\\tLoss: {}. Accuracy: {}'.format(loss_avg, accuracy_avg))\n",
    "print('Done Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "model_name_path = 'params\\\\tl_1.json'\n",
    "model_weights_path = 'params\\\\tl_1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(model_name_path, \"w\") as json_file:             \n",
    "     json_file.write(model_json) \n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_weights_path)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xi_test, yi_test = shuffle_matrix(x_test, y_test)    \n",
    "test_batches = mini_batch_generator(xi_test, yi_test, vocab, vocab_size, check, maxlen, batch_size=batch_size)\n",
    "test_accuracy = 0.0\n",
    "test_loss = 0.0\n",
    "test_step = 1\n",
    "for x_test_batch, y_test_batch in test_batches:\n",
    "        f_ev = model.test_on_batch(x_test_batch, y_test_batch)\n",
    "        test_loss += f_ev[0]\n",
    "        test_loss_avg = test_loss / test_step\n",
    "        test_accuracy += f_ev[1]\n",
    "        test_accuracy_avg = test_accuracy / test_step\n",
    "        test_step += 1\n",
    "    \n",
    "print('Loss: {}. Accuracy: {}\\n'.format(test_loss_avg, test_accuracy_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
