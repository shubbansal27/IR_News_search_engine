{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd C:\\\\Users\\\\FDUSER.M-1737.000\\\\dataset\\\\AGNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ag_data():\n",
    "    train = pd.read_csv('train.csv', header=None)\n",
    "    train = train.dropna()\n",
    "\n",
    "    x_train = train[1] + train[2]\n",
    "    #x_train = x_train[:2000]\n",
    "    x_train = np.array(x_train)\n",
    "    \n",
    "    \n",
    "    y_train = train[0] - 1\n",
    "    #y_train = y_train[:2000]\n",
    "    y_train = to_categorical(y_train)\n",
    "    \n",
    "     \n",
    "    \n",
    "    test = pd.read_csv('test.csv', header=None)\n",
    "    x_test = test[1] + test[2]\n",
    "    #x_test = x_test[2000:3000]\n",
    "    x_test = np.array(x_test)\n",
    "    \n",
    "\n",
    "    y_test = test[0] - 1\n",
    "    #y_test = y_test[2000:3000]\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mini_batch_generator(x, y, vocab, vocab_size, vocab_check, maxlen, batch_size=128):\n",
    "\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        x_sample = x[i:i + batch_size]\n",
    "        y_sample = y[i:i + batch_size]\n",
    "\n",
    "        input_data = encode_data(x_sample, maxlen, vocab, vocab_size,\n",
    "                                 vocab_check)\n",
    "\n",
    "        yield (input_data, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_data(x, maxlen, vocab, vocab_size, check):\n",
    "    #Iterate over the loaded data and create a matrix of size maxlen x vocabsize\n",
    "    #In this case that will be 1014x69. This is then placed in a 3D matrix of size\n",
    "    #data_samples x maxlen x vocab_size. Each character is encoded into a one-hot\n",
    "    #array. Chars not in the vocab are encoded into an all zero vector.\n",
    "\n",
    "    input_data = np.zeros((len(x), maxlen, vocab_size))\n",
    "    \n",
    "    for dix, sent in enumerate(x):\n",
    "        counter = 0\n",
    "        sent_array = np.zeros((maxlen, vocab_size))\n",
    "        chars = list(sent.lower().replace(' ', ''))\n",
    "        for c in chars:\n",
    "            if counter >= maxlen:\n",
    "                pass\n",
    "            else:\n",
    "                char_array = np.zeros(vocab_size, dtype=np.int)\n",
    "                if c in check:\n",
    "                    ix = vocab[c]\n",
    "                    char_array[ix] = 1\n",
    "                sent_array[counter, :] = char_array\n",
    "                counter += 1\n",
    "        input_data[dix, :, :] = sent_array\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function creates a vocab of characters.\n",
    "\n",
    "def create_vocab_set():\n",
    "    #This alphabet is 69 chars vs. 70 reported in the paper since they include two\n",
    "    # '-' characters. See https://github.com/zhangxiangxiao/Crepe#issues.\n",
    "\n",
    "    alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n",
    "                list(string.punctuation) + ['\\n'])\n",
    "    \n",
    "    vocab_size = len(alphabet)\n",
    "    \n",
    "    check = set(alphabet)\n",
    "\n",
    "    vocab = {}\n",
    "    reverse_vocab = {}\n",
    "    \n",
    "    for ix, t in enumerate(alphabet):\n",
    "        vocab[t] = ix\n",
    "        reverse_vocab[ix] = t\n",
    "\n",
    "    return vocab, reverse_vocab, vocab_size, check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_matrix(x, y):\n",
    "    stacked = np.hstack((np.matrix(x).T, y))\n",
    "    np.random.shuffle(stacked)\n",
    "    xi = np.array(stacked[:, 0]).flatten()\n",
    "    yi = np.array(stacked[:, 1:])\n",
    "\n",
    "    return xi, yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter, cat_output):\n",
    "    \n",
    "    #Define what the input shape looks like\n",
    "    inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')\n",
    "\n",
    "    #All the convolutional layers...\n",
    "    \n",
    "    conv = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[0], border_mode='valid', activation='relu',\n",
    "                         input_shape=(maxlen, vocab_size))(inputs)\n",
    "    \n",
    "    conv = MaxPooling1D(pool_length=3)(conv)\n",
    "\n",
    "    \n",
    "    conv1 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[1],\n",
    "                          border_mode='valid', activation='relu')(conv)\n",
    "    \n",
    "    conv1 = MaxPooling1D(pool_length=3)(conv1)\n",
    "\n",
    "    conv2 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[2],\n",
    "                          border_mode='valid', activation='relu')(conv1)\n",
    "\n",
    "    conv3 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[3],\n",
    "                          border_mode='valid', activation='relu')(conv2)\n",
    "\n",
    "    conv4 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[4],\n",
    "                          border_mode='valid', activation='relu')(conv3)\n",
    "\n",
    "    conv5 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[5],\n",
    "                          border_mode='valid', activation='relu')(conv4)\n",
    "    \n",
    "    conv5 = MaxPooling1D(pool_length=3)(conv5)\n",
    "    conv5 = Flatten()(conv5)\n",
    "\n",
    "    #Two dense layers with dropout of .5\n",
    "    z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv5))\n",
    "    z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z))\n",
    "\n",
    "    #Output dense layer with softmax activation\n",
    "    pred = Dense(cat_output, activation='softmax', name='output')(z)\n",
    "\n",
    "    model = Model(input=inputs, output=pred)\n",
    "\n",
    "    sgd = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam', # changed to adam\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model params\n",
    "\n",
    "#Filters for conv layers\n",
    "nb_filter = 256\n",
    "\n",
    "#Number of units in the dense layer\n",
    "dense_outputs = 1024\n",
    "\n",
    "#Conv layer kernel size\n",
    "filter_kernels = [7, 7, 3, 3, 3, 3]\n",
    "\n",
    "#Number of units in the final output layer. Number of classes.\n",
    "cat_output = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Maximum length. Longer gets chopped. Shorter gets padded.\n",
    "maxlen = 1014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(xt, yt), (x_test, y_test) = load_ag_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = encode_data(x_test, maxlen, vocab, vocab_size, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model(filter_kernels, dense_outputs, maxlen, vocab_size,nb_filter, cat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile/fit params\n",
    "batch_size = 80\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Fit model...')\n",
    "\n",
    "for e in range(nb_epoch):\n",
    "    \n",
    "    xi, yi = shuffle_matrix(xt, yt)\n",
    "    xi_test, yi_test = shuffle_matrix(x_test, y_test)\n",
    "    \n",
    "    batches = mini_batch_generator(xi, yi, vocab, vocab_size, check, maxlen, batch_size=batch_size)\n",
    "\n",
    "    test_batches = mini_batch_generator(xi_test, yi_test, vocab, vocab_size, check, maxlen, batch_size=batch_size)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    loss = 0.0\n",
    "    step = 1\n",
    "    print('Epoch: {}'.format(e))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for x_train, y_train in batches:\n",
    "        f = model.train_on_batch(x_train, y_train)\n",
    "        loss += f[0]\n",
    "        loss_avg = loss / step\n",
    "        accuracy += f[1]\n",
    "        accuracy_avg = accuracy / step\n",
    "        if step % 100 == 0:\n",
    "            print('  Step: {}'.format(step))\n",
    "            print('\\tLoss: {}. Accuracy: {}'.format(loss_avg, accuracy_avg))\n",
    "        step += 1\n",
    "\n",
    "    test_accuracy = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_step = 1\n",
    "    \n",
    "    for x_test_batch, y_test_batch in test_batches:\n",
    "        f_ev = model.test_on_batch(x_test_batch, y_test_batch)\n",
    "        test_loss += f_ev[0]\n",
    "        test_loss_avg = test_loss / test_step\n",
    "        test_accuracy += f_ev[1]\n",
    "        test_accuracy_avg = test_accuracy / test_step\n",
    "        test_step += 1\n",
    "    \n",
    "    print('Epoch {}. Loss: {}. Accuracy: {}\\n'.format(e, test_loss_avg, test_accuracy_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name_path = 'params\\\\AG_model3.json'\n",
    "model_weights_path = 'params\\\\AG_model_weights3.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(model_name_path, \"w\") as json_file:             \n",
    "     json_file.write(model_json) \n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_weights_path)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
